{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import wikipedia\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "from ssl import *\n",
    "from bs4 import BeautifulSoup\n",
    "from wikipedia.exceptions import PageError, DisambiguationError\n",
    "\n",
    "headers_Get = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "               'Accept-Language': 'en-US,en;q=0.5', 'Accept-Encoding': 'gzip, deflate', 'DNT': '1',\n",
    "               'Connection': 'keep-alive', 'Upgrade-Insecure-Requests': '1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrap_events(df):\n",
    "    \"\"\" This function will gather events data from the \n",
    "        website https://www.onthisday.com/ for all classes that\n",
    "        have the name event. \"\"\"\n",
    "    # Create a map to have to correct month names\n",
    "    month_map = {'Jan': 'January', 'Feb': 'February', 'Mar': 'March', 'Apr': 'April', \n",
    "                 'May': 'May', 'Jun': 'June', 'Jul': 'July', 'Aug': 'August',\n",
    "                 'Sep': 'September', 'Oct': 'October', 'Nov': 'November', 'Dec': 'December'}\n",
    "    year_range = [x for x in range(1965, 2016)]\n",
    "    for year in year_range:\n",
    "        s = requests.Session()\n",
    "        url = 'https://www.onthisday.com/date/{}'.format(year)\n",
    "        r = s.get(url, headers=headers_Get)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        # We get all events elements from the website \n",
    "        result_set = soup.find_all('li', {'class': 'event'})\n",
    "        for res in result_set:\n",
    "            text = res.get_text()\n",
    "            tokens = text.split(\" \")\n",
    "            month = tokens[0]\n",
    "            day = tokens[1]\n",
    "            content = \" \".join(tokens[2:])\n",
    "            df = df.append({'Year': year, 'Month': month_map[month],\n",
    "                            'Day': day, 'Content': content}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_wikipedia_values(df):\n",
    "    \"\"\" This function gather the url and summary of \n",
    "        related wikipedia article. \"\"\"\n",
    "    done = False\n",
    "    while not done:\n",
    "        try:\n",
    "            for i, row in df.iterrows():\n",
    "                if(pd.isna(row[4]) or pd.isna(row[5])):\n",
    "                    y = row[0]\n",
    "                    m = row[1]\n",
    "                    d = row[2]\n",
    "                    content = row[3]\n",
    "                    res = wikipedia.search(\"{} {} {} {}\".format(d, m, y, content), results=1)\n",
    "                    if(len(res) != 0):\n",
    "                        try:\n",
    "                            df.loc[df['Content'] == content, 'Wikipedia'] = wikipedia.page(res[0]).url\n",
    "                            df.loc[df['Content'] == content, 'Summary'] = wikipedia.summary(res[0])\n",
    "                        except PageError:\n",
    "                            pass\n",
    "                        except DisambiguationError:\n",
    "                            pass\n",
    "            done = True\n",
    "        except:\n",
    "            continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fill in url and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_event = pd.read_csv('datasets/events/events_full.csv', sep=',', index_col=0)\n",
    "except:\n",
    "    # We create an empty dataframe\n",
    "    columns = ['Year', 'Month', 'Day', 'Content', 'Wikipedia', 'Summary']\n",
    "    df_event = pd.DataFrame(columns=columns)\n",
    "    df_event = scrap_events(df_event)\n",
    "    df_event = fill_wikipedia_values(df_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the wikipedia url by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the missing entries, we found the wikipedia url by hand. Then we extract the article's summary for those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_nan = df_event[df_event['Summary'].isna()]\n",
    "print(\"We have {} missing summaries\".format(len(summaries_nan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "for i, row in summaries_nan.iterrows():\n",
    "    url = row.loc['Wikipedia']\n",
    "    title = url[30:]\n",
    "    page = wiki_wiki.page(title) \n",
    "    try:\n",
    "        if(page.exists):\n",
    "            df_event.loc[df_event['Wikipedia'] == url, 'Summary'] = page.summary\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_nan = df_event[df_event['Summary'].isna()]\n",
    "print(\"We have {} missing summaries\".format(len(summaries_nan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event.to_csv(\"datasets/events/events_full.csv\", sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
